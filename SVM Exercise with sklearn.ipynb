{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script gives a basic description on how to use sklearn using an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC,SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "# 訓練データとテストデータを分けるライブラリ\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読み込み\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create the data frame\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "c= pd.concat([pd.DataFrame(cancer.data),pd.DataFrame(cancer.target)], axis= 1)\n",
    "a=list(cancer.feature_names)\n",
    "a.append('cancer')\n",
    "c.columns=a\n",
    "\n",
    "# Next, we take out the x and y , and scale the data\n",
    "y = c['cancer']\n",
    "x = preprocessing.scale(c.drop('cancer', axis = 1),axis=0)\n",
    "\n",
    "# Next, we split model into tran and test \n",
    "x_train, x_verify, y_train, y_verify=train_test_split(x,y, test_size=0.5,random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics about the sklearn specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if\n",
    "you scale the data using the StandardScaler. \n",
    "2.Also make sure you set the loss hyperparameter to \"hinge\", as it is not the default value. By setting this, you are using soft margin\n",
    "3.Finally, for better performance, you should set the dual hyperparameter to False, unless there are more features than training instances \n",
    "some error:\n",
    "Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the model\n",
    "#If your SVM model is overfitting, you can try regularizing it by reducing C.\n",
    "# loss function specify the loss from misclassfication\n",
    "\n",
    "model = LinearSVC(C=1, loss='hinge')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_verify,y_verify)\n",
    "\n",
    "# use cross validation to see the accuracy, recall, precision \n",
    "cross_val_score(model,x,y,cv=3,scoring='precision')\n",
    "cross_val_score(model,x,y,cv=3,scoring='recall')\n",
    "\n",
    "# can also get the prediction \n",
    "# unlike the logistic regression, the svm classifier sklearn command cannot return the 'possibility' by default\n",
    "\n",
    "\n",
    "a=model.predict(x_verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want the svm to be able to get the probability, you should  you need to set its probability hyper‐\n",
    "parameter to True (this will make the SVC class use cross-validation to estimate class\n",
    "probabilities, slowing down training, and it will add a predict_proba() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tackle Linearly Unseparability 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, when the data itself is hard to separat linearly, a good method is to use kernel function to map them to higher dimension.\n",
    "The following block shows a svc with polynomial kernel. \n",
    "If your model is overfitting, you might want to\n",
    "reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\n",
    "it. The hyperparameter coef0 controls how much the model is influenced by highdegree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48894454918224145\n",
      "0.49102583009940254\n",
      "0.4917233013423949\n",
      "0.490334412453506\n",
      "0.4874925933611028\n",
      "0.4867077701186449\n"
     ]
    }
   ],
   "source": [
    "# we next do an excercise to compare the performance of degree of polynomial kernel . \n",
    "\n",
    "from statistics import mean\n",
    "for d in [1,2,3,4,5,6]:\n",
    "    model= SVC(kernel=\"poly\", degree=d, coef0=1, C=1)\n",
    "    pre_score= cross_val_score(model,x,y,cv=3,scoring='precision')\n",
    "    rec_score = cross_val_score(model,x,y,cv=3,scoring='recall')\n",
    "    print (mean(list(map(lambda x,y: x*y/(x+y), pre_score, rec_score))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tackle Linearly Unseparability 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use rbf, which can map the original data into infinite dimension. \n",
    "γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it; if it is underfitting,\n",
    "you should increase it (similar to the C hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38553005786015493\n",
      "0.38553005786015493\n",
      "0.38553005786015493\n",
      "0.38553005786015493\n",
      "0.38553005786015493\n",
      "0.38553005786015493\n",
      "0.38553005786015493\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "for g in [1,2,3,4,5,6,7]:\n",
    "    model= SVC(kernel=\"rbf\",gamma=g , C=0.001)\n",
    "    pre_score= cross_val_score(model,x,y,cv=3,scoring='precision')\n",
    "    rec_score = cross_val_score(model,x,y,cv=3,scoring='recall')\n",
    "    print (mean(list(map(lambda x,y: x*y/(x+y), pre_score, rec_score))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(kernel=\"linear\")), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should also try the Gaussian RBF kernel; it works well in most cases. Then if you have spare time and computing power, you can experiment with a few other kernels, using cross-validation and grid search.You’d want to experiment like that especially if there are kernels specialized for your training set’s data structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
