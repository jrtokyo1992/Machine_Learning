{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the main code for generating Gradient Boost for classification tree.\n",
    "The basic idea is from \n",
    "https://www.youtube.com/watch?v=jxuNLH5dXCs&t=2s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "data= pd.read_excel('C:/Users/shufe/Dropbox/ds new/pwchkt.xlsx')\n",
    "data.isnull().any(axis=1) \n",
    "data=data.dropna(how='any')\n",
    "data=data[['age','creditamount','guarantors','housing','foreigner','creditrating']]\n",
    "data.loc[(data.creditrating == 2),'creditrating']=0\n",
    "data_features_original = list(data.columns) \n",
    "y='creditrating'\n",
    "data_features_original.remove(y)\n",
    "# we also need to create a dictionary indicating whether the feature is discrete or not\n",
    "discrete={'age':'no','creditamount':'no','guarantors':'yes','housing':'yes','foreigner':'yes','creditrating':'yes'}\n",
    "data_features=data_features_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoost (data , max_tree_number,learn_rate ):\n",
    "    n=len(data)\n",
    "    current_data = data.copy()\n",
    "    current_tree_series = []\n",
    "    current_tree_series.append( math.log( sum ( data [ y ] == 1 )/ sum ( data[ y ] == 0 ) ) )\n",
    "    current_odd = [math.log( sum ( data [ y ] == 1 )/ sum ( data[ y ] == 0 ) ) for i in range(0,n)]\n",
    "    current_data['pre_predict']=[  math.exp(f)/(math.exp(f)+1)     for f in current_odd]\n",
    "    while (len(current_tree_series)<=max_tree_number):\n",
    "       \n",
    "        data_features = data_features_original.copy()\n",
    "       # thorough this step you get a new tree. the leaves of the tree stores odd value\n",
    "        new_tree = createTree ( current_data, 1, data_features) # a tree always start from first layer\n",
    "      # add the newly generated tree into the current tree respository\n",
    "        current_tree_series.append ( new_tree )\n",
    "       # produce new odd value\n",
    "        new_odd = [ output ( current_data.loc[i],new_tree ) for  i in range(0,n) ]\n",
    "       # add to the current odd value with a learning rate\n",
    "        current_odd = list(map(lambda x,y:x+y, current_odd , [j * learn_rate for j in new_odd] ))\n",
    "       # using the current odd value, you can produce a up-to-now prediction for each individual\n",
    "        current_data['pre_predict']=[  math.exp(f)/(math.exp(f)+1)   for f in current_odd]\n",
    "       # print(sum(list(map(lambda a,b: abs(a-b), current_data[y],current_data['pre_predict']))))\n",
    "    return current_tree_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classifier: Tree Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree ( Dataset, layer,features):\n",
    "    # Dataset contains the real y and the previous predict (p, not odd value)\n",
    "    # we first need to check whether the layer has already reached the max\n",
    "    \n",
    "    if ( layer == maxlayer ):\n",
    "        # you have already reached the deepest. \n",
    "        # you only have to return the gama, as the final result of this leaf.\n",
    "        Tree = loss_cal ( Dataset[y] , Dataset['pre_predict'])[0]\n",
    "    else:\n",
    "        best_feature,best_threshold = BestFeature ( Dataset, features)\n",
    "        Tree = {}  # means that you want to generate a new son tree\n",
    "        # the new son tree should contain the following features:\n",
    "        Tree['spInd'] = best_feature # the feature you use to split the data\n",
    "        featuresNext = features\n",
    "        featuresNext.remove ( best_feature )  # this feature would no longer be used in the next trees\n",
    "        Tree['spVal'] = best_threshold # the threshold value \n",
    "        data_left, data_right = SplitDataSet(\n",
    "            Dataset,best_feature,best_threshold)[1:3]\n",
    "        Tree['left'] = createTree ( data_left, layer+1,featuresNext)\n",
    "        Tree['right'] = createTree ( data_right, layer+1,featuresNext)\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq=8\n",
    "def BestFeature ( Dataset, unvisited_feature ):\n",
    "    # want to return the best feature and the resulting data split\n",
    "    # but before this , you can already calculate the the entropy of the current data\n",
    "    threshold=[]\n",
    "    loss=[]\n",
    "    \n",
    "    for feature in unvisited_feature:\n",
    "        if discrete[feature]=='yes': # this is discrete \n",
    "            # feature is name\n",
    "            # next, for each level in the feature_level, get\n",
    "            loss.append ( SplitDataSet ( Dataset,feature, 0.5)[0])\n",
    "            threshold.append(0.5)\n",
    "        else: # the feature is continuous \n",
    "            # we first generate a threshold sequence \n",
    "            thres_seq = [   min(Dataset[feature])+i*(max(Dataset[feature])-min(Dataset[feature]))/(n_seq-1) \n",
    "                      for i in range(1,n_seq-1)   ]\n",
    "            loss_seq= [ SplitDataSet(Dataset,feature, x) [0] for x in thres_seq]\n",
    "            loss.append ( min ( loss_seq ))\n",
    "            threshold.append ( thres_seq [ loss_seq.index( min ( loss_seq) ) ])\n",
    "        \n",
    "    # we complete the loop with loss series\n",
    "    # we now return the best feature and the corresponding threshold \n",
    "    best_feature = unvisited_feature [ loss.index(min(loss)) ]\n",
    "    best_threshold = threshold [ loss.index(min(loss)) ]\n",
    "    # we also return the splitted dat\n",
    "    return best_feature,best_threshold\n",
    "    # best_feature is a string. best_threshold is a value. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataSet ( Dataset,feature, value ):\n",
    "# we only consider the binary classification case\n",
    "# given the dataset, feature and value, this function returns splitted data and the resulting entropy!\n",
    "    \n",
    "    Data_left = Dataset[ Dataset[feature]<value ]\n",
    "    Data_right = Dataset[ Dataset[feature]>=value ]\n",
    "    \n",
    "# we want to calculate the loss function the data_left, data_right\n",
    "    \n",
    "    if (len(Data_left)*len(Data_right)>0): \n",
    "        Total_loss = loss_cal ( Data_left[y] , Data_left['pre_predict']\n",
    "                          )[1] + loss_cal ( Data_right[y] , Data_right['pre_predict'] )[1]\n",
    "    else: # if any of the sub data is null, then the current (feature, value) split is not valid. set a inf value to the loss function\n",
    "        Total_loss=float(\"inf\")\n",
    "    \n",
    "    return Total_loss,Data_left, Data_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function due to classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def loss_cal ( y_data , pre_predict ):\n",
    "    # this function gives you the output of loss function as well as the gamma, the odd ratio at the leaf.\n",
    "    # y_data is already the residual!\n",
    "    residual=y_data-pre_predict\n",
    "    gama = sum ( residual ) / ( sum ( [  p*(1-p)  for p in pre_predict ] )  )\n",
    "    # the expression for gama is actually the solution to the maximization of the following loss function.\n",
    "    loss = sum ( list ( map( lambda y,p :-y*math.log(p)-(1-y)*math.log(1-p),y_data, pre_predict) ))\n",
    "    return gama , loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the Odd ratio of a sample given a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sample, tree):\n",
    "    if (sample[tree['spInd']]<tree['spVal']):\n",
    "        odd = output(sample,tree['left']) if type(tree['left']).__name__=='dict' else tree['left']\n",
    "    else:\n",
    "        odd = output(sample,tree['right']) if type(tree['right']).__name__=='dict' else  tree['right']\n",
    "    return (odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlayer=3\n",
    "GradientBoost (data , 300,0.3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
