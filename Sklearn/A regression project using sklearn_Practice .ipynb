{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This code simply goes through the main procedure of doing model traning, selection and evaluation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Input and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import scipy as sp\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import mean\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import  DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import f_oneway\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# get the data\n",
    "import requests, zipfile\n",
    "import io\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n",
    "res = requests.get(url).content\n",
    "auto_raw = pd.read_csv(io.StringIO(res.decode('utf-8')), header=None)\n",
    "\n",
    "# set columns\n",
    "auto_raw.columns =['symboling','normalized-losses','make','fuel-type' ,'aspiration','num-of-doors',\n",
    "                            'body-style','drive-wheels','engine-location','wheel-base','length','width','height',\n",
    "                            'curb-weight','engine-type','num-of-cylinders','engine-size','fuel-system','bore',\n",
    "                            'stroke','compression-ratio','horsepower','peak-rpm','city-mpg','highway-mpg','price']\n",
    "\n",
    "# get abnormal values\n",
    "auto_raw.isin(['?']).sum()\n",
    "# therefore we need to change. replace the ? with NA (To realize this, use np.nan)\n",
    "auto_raw=auto_raw.replace(\"?\", np.nan).dropna()\n",
    "\n",
    "# get numerical and category variable\n",
    "numerical = auto_raw.select_dtypes(exclude=['object'])\n",
    "category = auto_raw.select_dtypes('object')\n",
    "category_copy = category[:] # This step seems important. what is the reason?\n",
    "# the following four variables, although looking like 'numeric', actually are 'character'. we need to convert then into numeric.\n",
    "category_copy[['horsepower', 'price', 'bore','stroke','normalized-losses','peak-rpm']] = category_copy[['horsepower', 'price', 'bore','stroke','normalized-losses','peak-rpm']].apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symboling</th>\n",
       "      <th>wheel-base</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>curb-weight</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-type_ohcv</th>\n",
       "      <th>num-of-cylinders_five</th>\n",
       "      <th>num-of-cylinders_four</th>\n",
       "      <th>num-of-cylinders_six</th>\n",
       "      <th>num-of-cylinders_three</th>\n",
       "      <th>fuel-system_2bbl</th>\n",
       "      <th>fuel-system_idi</th>\n",
       "      <th>fuel-system_mfi</th>\n",
       "      <th>fuel-system_mpfi</th>\n",
       "      <th>fuel-system_spdi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>99.8</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.2</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2337</td>\n",
       "      <td>109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>99.4</td>\n",
       "      <td>176.6</td>\n",
       "      <td>66.4</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2824</td>\n",
       "      <td>136</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>105.8</td>\n",
       "      <td>192.7</td>\n",
       "      <td>71.4</td>\n",
       "      <td>55.7</td>\n",
       "      <td>2844</td>\n",
       "      <td>136</td>\n",
       "      <td>8.5</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>105.8</td>\n",
       "      <td>192.7</td>\n",
       "      <td>71.4</td>\n",
       "      <td>55.9</td>\n",
       "      <td>3086</td>\n",
       "      <td>131</td>\n",
       "      <td>8.3</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>101.2</td>\n",
       "      <td>176.8</td>\n",
       "      <td>64.8</td>\n",
       "      <td>54.3</td>\n",
       "      <td>2395</td>\n",
       "      <td>108</td>\n",
       "      <td>8.8</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-1</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>2952</td>\n",
       "      <td>141</td>\n",
       "      <td>9.5</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-1</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.8</td>\n",
       "      <td>55.5</td>\n",
       "      <td>3049</td>\n",
       "      <td>141</td>\n",
       "      <td>8.7</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>3012</td>\n",
       "      <td>173</td>\n",
       "      <td>8.8</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-1</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>3217</td>\n",
       "      <td>145</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-1</td>\n",
       "      <td>109.1</td>\n",
       "      <td>188.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>55.5</td>\n",
       "      <td>3062</td>\n",
       "      <td>141</td>\n",
       "      <td>9.5</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     symboling  wheel-base  length  width  height  curb-weight  engine-size  \\\n",
       "3            2        99.8   176.6   66.2    54.3         2337          109   \n",
       "4            2        99.4   176.6   66.4    54.3         2824          136   \n",
       "6            1       105.8   192.7   71.4    55.7         2844          136   \n",
       "8            1       105.8   192.7   71.4    55.9         3086          131   \n",
       "10           2       101.2   176.8   64.8    54.3         2395          108   \n",
       "..         ...         ...     ...    ...     ...          ...          ...   \n",
       "200         -1       109.1   188.8   68.9    55.5         2952          141   \n",
       "201         -1       109.1   188.8   68.8    55.5         3049          141   \n",
       "202         -1       109.1   188.8   68.9    55.5         3012          173   \n",
       "203         -1       109.1   188.8   68.9    55.5         3217          145   \n",
       "204         -1       109.1   188.8   68.9    55.5         3062          141   \n",
       "\n",
       "     compression-ratio  city-mpg  highway-mpg  ...  engine-type_ohcv  \\\n",
       "3                 10.0        24           30  ...                 0   \n",
       "4                  8.0        18           22  ...                 0   \n",
       "6                  8.5        19           25  ...                 0   \n",
       "8                  8.3        17           20  ...                 0   \n",
       "10                 8.8        23           29  ...                 0   \n",
       "..                 ...       ...          ...  ...               ...   \n",
       "200                9.5        23           28  ...                 0   \n",
       "201                8.7        19           25  ...                 0   \n",
       "202                8.8        18           23  ...                 1   \n",
       "203               23.0        26           27  ...                 0   \n",
       "204                9.5        19           25  ...                 0   \n",
       "\n",
       "     num-of-cylinders_five  num-of-cylinders_four  num-of-cylinders_six  \\\n",
       "3                        0                      1                     0   \n",
       "4                        1                      0                     0   \n",
       "6                        1                      0                     0   \n",
       "8                        1                      0                     0   \n",
       "10                       0                      1                     0   \n",
       "..                     ...                    ...                   ...   \n",
       "200                      0                      1                     0   \n",
       "201                      0                      1                     0   \n",
       "202                      0                      0                     1   \n",
       "203                      0                      0                     1   \n",
       "204                      0                      1                     0   \n",
       "\n",
       "     num-of-cylinders_three  fuel-system_2bbl  fuel-system_idi  \\\n",
       "3                         0                 0                0   \n",
       "4                         0                 0                0   \n",
       "6                         0                 0                0   \n",
       "8                         0                 0                0   \n",
       "10                        0                 0                0   \n",
       "..                      ...               ...              ...   \n",
       "200                       0                 0                0   \n",
       "201                       0                 0                0   \n",
       "202                       0                 0                0   \n",
       "203                       0                 0                1   \n",
       "204                       0                 0                0   \n",
       "\n",
       "     fuel-system_mfi  fuel-system_mpfi  fuel-system_spdi  \n",
       "3                  0                 1                 0  \n",
       "4                  0                 1                 0  \n",
       "6                  0                 1                 0  \n",
       "8                  0                 1                 0  \n",
       "10                 0                 1                 0  \n",
       "..               ...               ...               ...  \n",
       "200                0                 1                 0  \n",
       "201                0                 1                 0  \n",
       "202                0                 1                 0  \n",
       "203                0                 0                 0  \n",
       "204                0                 1                 0  \n",
       "\n",
       "[159 rows x 54 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto  = pd.concat( [numerical , category_copy] , axis = 1 )\n",
    "numerical_real = auto.select_dtypes (exclude = ['object'])\n",
    "# turn categoryt variables into the dummy variables\n",
    "category_dummy = pd.get_dummies(auto.select_dtypes('object'),drop_first=True)\n",
    "auto_final = pd.concat( [numerical_real , category_dummy] , axis = 1 )\n",
    "x = auto_final.drop('price',axis=1)\n",
    "x_varname = x.columns\n",
    "y = auto_final['price']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following function, I try filter method and embedded method.\n",
    "\n",
    "def feature_selection(feature_number):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(x)\n",
    "    x_for_selection = scaler.transform (x)\n",
    "    var_selected = np.empty(0)\n",
    "    \n",
    "    # first try fitler selection method:\n",
    "    for method in [f_regression, mutual_info_regression]:\n",
    "        select = SelectKBest(method, k=feature_number)\n",
    "        select.fit(x_for_selection,y)\n",
    "        var_selected =np.concatenate([ var_selected, x_varname[select.get_support(indices=True) ]], 0)\n",
    "    \n",
    "    # next try the embedded method: \n",
    "    # Lasso with l1 term can filter out the coefficient near zero. SVR does not have coef_ or feature_importance (?)\n",
    "    # decision tree can output the feature importance, and can filter out those features with low importance.\n",
    "    for model in [Lasso(alpha=0.1, random_state=0,max_iter = 2000),\n",
    "                  RandomForestRegressor(n_estimators=100, max_leaf_nodes=16, n_jobs=-1,random_state =0 ),\n",
    "                  DecisionTreeRegressor(criterion='mse', max_depth=4, random_state=0),\n",
    "                 AdaBoostRegressor (n_estimators=100,learning_rate=0.5)]:\n",
    "        model_fit = model.fit(x_for_selection, y)\n",
    "        #lsvc = LinearSVC( penalty=\"l1\", dual=False).fit(x, y)\n",
    "        select = SelectFromModel(model_fit, prefit=True)\n",
    "        var_selected =np.concatenate([ var_selected, x_varname[select.get_support(indices=True) ]], 0)\n",
    "    \n",
    "    # now calculate the frequency of each variable in var_selected\n",
    "    unique_elements, counts_elements = np.unique(var_selected, return_counts=True)\n",
    "    counts_elements,unique_elements = (list(t) for t in zip(*sorted(zip( list(counts_elements), list(unique_elements) ))))\n",
    "    unique_elements.reverse()\n",
    "    counts_elements.reverse()\n",
    "    return unique_elements[0:feature_number],counts_elements[0:feature_number]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = feature_selection(8) [0]\n",
    "# finally we finished with the selection.\n",
    "x_final = x.loc[:,x.columns.isin(feature_name)]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_final)\n",
    "x_final = scaler.transform (x_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Selection via hyperparameter tuning\n",
    "We next use GridsearchCV in sklearn to do hyperparameter tuning for each type algorithm. We consider the following algorithms: random forest, lasso regression, adaboost, gradient boost, and svr\n",
    "<p>the following links summarizes some general rule of thumbs when tuning parameters.\n",
    "https://qiita.com/R1ck29/items/50ba7fa5afa49e334a8f </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this function to perform hyperparameter tuning for a given algorithm.\n",
    "# take parameter grid, model, and scoring as input, the function outputs the 'best' model with best score\n",
    "def grid_search (param_grid, clf, score):\n",
    "    # do not use repeatedstratifiedkfold heare.\n",
    "    my_cv= RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=my_cv,scoring=score,return_train_score=True)\n",
    "    grid_search.fit(x_final, y)\n",
    "    return grid_search.best_estimator_,grid_search.best_score_,grid_search.cv_results_['std_test_score'][grid_search.best_index_] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function get best model from each algorithm \n",
    "# through these methods, I can get the votes for each features. I select those features with most votes\n",
    "def ModelSelection (criterion):\n",
    "    score = criterion\n",
    "    \n",
    "    # we first fine tune randomforest \n",
    "    param_grid = [{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "     ]\n",
    "    clf = RandomForestRegressor() \n",
    "    rdf_best = grid_search (param_grid, clf, score)\n",
    "\n",
    "    # we next do svr \n",
    "    param_grid = [\n",
    "     { 'C': [0.1, 0.3, 0.6, 1], 'kernel': ['linear','rbf']},\n",
    "     ]\n",
    "    clf = SVR() \n",
    "    svr_best = grid_search (param_grid, clf, score) \n",
    "\n",
    "    # we next fine tune adaboost\n",
    "    # gridsearchCV is also useful for ensemble learning. \n",
    "    # here, notice that you may want to tune parameters both for the esembler (learning rate, number of estimators, and etc..) and the parameter \n",
    "    # for the the base estimator. you can use 'base_estimator__HyperparameterName' to tell sklearn that you are specifying the hyperparameter of base model.\n",
    "    # see https://stackoverflow.com/questions/32210569/using-gridsearchcv-with-adaboost-and-decisiontreeclassifier\n",
    "    # This is also time consuming. \n",
    "    param_grid = [\n",
    "     { 'base_estimator__max_depth': [1,2,3,4],'n_estimators':[50,75,100,125],'learning_rate':[0.1,0.3,0.5]}\n",
    "     ]\n",
    "    dtr = DecisionTreeRegressor()\n",
    "    clf = AdaBoostRegressor(base_estimator = dtr) \n",
    "    adb_best = grid_search (param_grid, clf, score) \n",
    "\n",
    "    # We next fine-tune the gradientboost. \n",
    "    # notice that gradient boost use decision tree as base estimator, so its hyperparameters naturally contains the hyperparameters of tree.\n",
    "    # therefore there is no worry about the specification of 'base_estimator'\n",
    "    param_grid = [\n",
    "     { 'max_depth': [1,3,5],'n_estimators':[50,75,100,125],'learning_rate':[0.05,0.1,0.15,0.2]}\n",
    "     ]\n",
    "    clf = GradientBoostingRegressor() \n",
    "    gdb_best = grid_search (param_grid, clf, score) \n",
    "    \n",
    "    # Finally we fine tune the lasso\n",
    "    param_grid = {\n",
    "    'alpha' : [  0.25, 0.5, 0.75,1]\n",
    "    }\n",
    "    ls = Lasso(max_iter = 2000)\n",
    "    clf = Lasso()\n",
    "    ls_best = grid_search (param_grid, clf, score)\n",
    "    \n",
    "    # Here we do not consider svc, since it is super time consuming. In practice, however, we can use RandomizedSearchCV to choose the best\n",
    "    # hyperparameters for svc\n",
    "    \n",
    "    return rdf_best, svr_best,adb_best, gdb_best, ls_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>performance_mean</th>\n",
       "      <th>performance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeRegressor(max_features=2, random_...</td>\n",
       "      <td>0.886084</td>\n",
       "      <td>0.043629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR(C=1, kernel='linear')</td>\n",
       "      <td>-0.159987</td>\n",
       "      <td>0.088055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=4, random_sta...</td>\n",
       "      <td>0.859909</td>\n",
       "      <td>0.044611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.873962</td>\n",
       "      <td>0.032291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>0.766220</td>\n",
       "      <td>0.078314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               MODEL  performance_mean  \\\n",
       "0  (DecisionTreeRegressor(max_features=2, random_...          0.886084   \n",
       "1                          SVR(C=1, kernel='linear')         -0.159987   \n",
       "2  (DecisionTreeRegressor(max_depth=4, random_sta...          0.859909   \n",
       "3  ([DecisionTreeRegressor(criterion='friedman_ms...          0.873962   \n",
       "4                                     Lasso(alpha=1)          0.766220   \n",
       "\n",
       "   performance_std  \n",
       "0         0.043629  \n",
       "1         0.088055  \n",
       "2         0.044611  \n",
       "3         0.032291  \n",
       "4         0.078314  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result = ModelSelection('r2')\n",
    "# show the results in data frame\n",
    "pd.DataFrame(model_result,columns=['MODEL', 'performance_mean', 'performance_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model(Algorithm) Comparisons using statistical tests\n",
    "We now turn to statistical tests to compare the models we selected from stage 3.\n",
    "<p>\n",
    "main reference: Chapter 6 in 'evaluating learning algorithms a classification perspective'</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.Comparing two algorithms using 5*2 cv\n",
    "In general, \n",
    "1. if you evaluate the performance of two classifiers over multiple domain, then 5*2 cv is a good choice. Paired t test via resampling is also a method, but often times the assumption for paired t test is violated. \n",
    "2. If you evaluate the performance of two classifier over one domain, then Mc Nemar contingency table test is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeRegressor(max_features=2, random_...</td>\n",
       "      <td>SVR(C=1, kernel='linear')</td>\n",
       "      <td>0.00108453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(DecisionTreeRegressor(max_features=2, random_...</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=4, random_sta...</td>\n",
       "      <td>0.699242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(DecisionTreeRegressor(max_features=2, random_...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.824972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(DecisionTreeRegressor(max_features=2, random_...</td>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>0.0241685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVR(C=1, kernel='linear')</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=4, random_sta...</td>\n",
       "      <td>0.00190213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVR(C=1, kernel='linear')</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.00223631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR(C=1, kernel='linear')</td>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>0.0044257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=4, random_sta...</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>0.621279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=4, random_sta...</td>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>0.0267478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>0.00402075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model1  \\\n",
       "0  (DecisionTreeRegressor(max_features=2, random_...   \n",
       "1  (DecisionTreeRegressor(max_features=2, random_...   \n",
       "2  (DecisionTreeRegressor(max_features=2, random_...   \n",
       "3  (DecisionTreeRegressor(max_features=2, random_...   \n",
       "4                          SVR(C=1, kernel='linear')   \n",
       "5                          SVR(C=1, kernel='linear')   \n",
       "6                          SVR(C=1, kernel='linear')   \n",
       "7  (DecisionTreeRegressor(max_depth=4, random_sta...   \n",
       "8  (DecisionTreeRegressor(max_depth=4, random_sta...   \n",
       "9  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "\n",
       "                                              model2     p-value  \n",
       "0                          SVR(C=1, kernel='linear')  0.00108453  \n",
       "1  (DecisionTreeRegressor(max_depth=4, random_sta...    0.699242  \n",
       "2  ([DecisionTreeRegressor(criterion='friedman_ms...    0.824972  \n",
       "3                                     Lasso(alpha=1)   0.0241685  \n",
       "4  (DecisionTreeRegressor(max_depth=4, random_sta...  0.00190213  \n",
       "5  ([DecisionTreeRegressor(criterion='friedman_ms...  0.00223631  \n",
       "6                                     Lasso(alpha=1)   0.0044257  \n",
       "7  ([DecisionTreeRegressor(criterion='friedman_ms...    0.621279  \n",
       "8                                     Lasso(alpha=1)   0.0267478  \n",
       "9                                     Lasso(alpha=1)  0.00402075  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for comparing two regressors using 5*2 cv\n",
    "# see https://machinelearningmastery.com/hypothesis-test-for-comparing-machine-learning-algorithms/\n",
    "# Also see http://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "t, p = paired_ttest_5x2cv(estimator1=clf1,estimator2=clf2,X=x_final, y=y,random_seed=1)\n",
    "ttest_result =[ [model_result[i][0], model_result[j][0],   paired_ttest_5x2cv(estimator1=model_result[i][0],estimator2=model_result[j][0],X=x_final, y=y,random_seed=1)[1]] \\\n",
    "for i in range(0,len(model_result)-1)  for j in range(i+1,len(model_result)) ]\n",
    "ttest_table = pd.DataFrame(np.vstack(ttest_result), columns= ['model1','model2','p-value'])\n",
    "ttest_table\n",
    "# however, pair wise comparison using t test may be misleading sometimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Comparing multiple algorithms \n",
    "The following code block is the functions for multiple classifier comparison and post-hoc test.\n",
    "1. Parametric method: ANOVA, followed by Tukey test as post-hoc test. \n",
    "2. Nonparametric method: Friedmand test, followed by Nemenyi test as post-hoc test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice : I only output the statisitics. need to check the threhold value table to determine whether to reject or accept the null hypothesis.\n",
    "#The algorithm is based on the 6.7.1 of 'evaluating learning algorithms a classification perspective'\n",
    "def multiple_clf_compare_anova(clf_list,n):\n",
    "    k = len (clf_list)\n",
    "    my_cv = KFold(n_splits=n, shuffle = False)\n",
    "    pfm = [ cross_val_score (model, x_final, y, cv = my_cv) for model in clf_list  ]\n",
    "    pfm_table = pd.DataFrame(np.vstack(pfm)).T\n",
    "    # we get the results for ANOVA test. \n",
    "    # anova = f_oneway(*pfm)\n",
    "    pfm_stack = pfm_table.stack ()\n",
    "    ss_total = sum([(x-mean(pfm_stack ))**2 for x in pfm_stack])\n",
    "    ss_pfm =   n* sum ([(x-mean(pfm_stack))**2   for x in  pfm_table.mean (axis = 0)])\n",
    "    ss_block = k * sum ([(x-mean(pfm_stack))**2   for x in  pfm_table.mean (axis = 1)])\n",
    "    ss_error = ss_total - (ss_pfm + ss_block)\n",
    "    ms_pfm , ms_error = ss_pfm /(k-1) , ss_error /((n-1)*(k-1))\n",
    "    F = ms_pfm / ms_error\n",
    "    return F,pfm_table, ms_error\n",
    "\n",
    "# The algorithm is based on the 6.7.5 of 'evaluating learning algorithms a classification perspective'\n",
    "def multiple_clf_compare_friedman(clf_list,n):\n",
    "    my_cv = KFold(n_splits=n, shuffle = False)\n",
    "    pfm = [ cross_val_score (model, x_final, y, cv = my_cv) for model in clf_list  ]\n",
    "    # we get results from the friedman test \n",
    "    # please check chapter 6 of \n",
    "    pfm_table = pd.DataFrame(np.vstack(pfm)).T\n",
    "    rank_table = pfm_table.rank(1, ascending=False, method='average')\n",
    "    k = len (clf_list)\n",
    "    friedman = 12* (np.square (rank_table.sum(axis = 0) ).sum())/(n * k * (k+1) )- 3 * n * (k +1)\n",
    "    return friedman,rank_table\n",
    "\n",
    "# if the friedman test shows that the there is difference between multiple classifiers, then do the post-doc test\n",
    "#  The algorithm is based on the 6.7.7 of 'evaluating learning algorithms a classification perspective'\n",
    "def post_hoc_anova (pfm_table,ms_error):\n",
    "    n,k =pfm_table.shape\n",
    "    avg_pfm = pfm_table.mean (axis = 0)\n",
    "    divide = (ms_error/n)**0.5\n",
    "    df = (n-1)*(k-1)\n",
    "    return [ [i,j, (avg_pfm[i]-avg_pfm[j])/ divide ]  for i in range (0,k-1) for j in range (i+1,k)], df,k\n",
    "\n",
    "#  The algorithm is based on the 6.7.8 of 'evaluating learning algorithms a classification perspective'\n",
    "def post_hoc_friedman(rank_table):\n",
    "    n,k =rank_table.shape\n",
    "    avg_rank = rank_table.mean(axis = 0)  # for each classifier, get its average rank\n",
    "    divide =( k*(k+1)/(6*n) )**0.5\n",
    "    df = (n-1)*(k-1)\n",
    "    return [ [i,j, (avg_rank[i]-avg_rank[j])/ divide ]  for i in range (0,k-1) for j in range (i+1,k)], df,k\n",
    "    # the degree of freedom is (n-1)*(k-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.840000000000003,\n",
       "      0    1    2    3    4\n",
       " 0  2.0  5.0  1.0  3.0  4.0\n",
       " 1  3.0  5.0  2.0  4.0  1.0\n",
       " 2  4.0  5.0  3.0  1.0  2.0\n",
       " 3  2.0  5.0  1.0  3.0  4.0\n",
       " 4  4.0  5.0  1.0  2.0  3.0\n",
       " 5  4.0  5.0  2.0  3.0  1.0\n",
       " 6  1.0  5.0  3.0  2.0  4.0\n",
       " 7  1.0  2.0  4.0  5.0  3.0\n",
       " 8  3.0  5.0  1.0  2.0  4.0\n",
       " 9  2.0  5.0  3.0  4.0  1.0)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_list =[x[0] for x in model_result]\n",
    "a=multiple_clf_compare_friedman (clf_list,10)\n",
    "print('the chi2 statistic is',a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Appendix \n",
    "### A. More on the Feature selection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Filter Method\n",
    "I can use feature_selection library from sklearn. class SelectKBest provides the tool to find out the k 'best' variables, in the sense that they have the highest correlation with target. Since I am dealing with binary target, I can use chi2, mutual_info_classif (mutual infomration) or f_classif (ANOVA) criteria to select best feature .\n",
    "see https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import  f_regression, mutual_info_regression\n",
    "# write a simple function that, use method and desired feature number as input\n",
    "# output the x_new (feature data) and x_varname_selected (feature name)\n",
    "def featureSelect (method_input, number_of_feature):\n",
    "    select = SelectKBest(method_input, k=number_of_feature)\n",
    "    x_new = select.fit_transform(x_final, y)\n",
    "    x_varname_selected =x_varname[select.get_support(indices=True) ]\n",
    "    return x_new, x_varname_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'education-num',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'marital-status_ Married-civ-spouse',\n",
       " 'marital-status_ Never-married',\n",
       " 'relationship_ Own-child']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= featureSelect(mutual_info_classif, 8)\n",
    "list(a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Recursive elimination\n",
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "Although this method works during the data training, we can still use this method to precheck the potential good features. \n",
    "however, when specifying the model, we must specify the hyperparameters. Therefore, we can actually choose number of features and model hyperparameters simultaneously. \n",
    "This method is super time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "# This is super time-consuming \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "selector = RFE(svc, n_features_to_select=8, step=2)\n",
    "selector.fit(x,y)\n",
    "selector.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Select from model (Embedded Method)\n",
    "\n",
    "<p>Here are some \n",
    "the following link is clear and clean.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.Model selection using the L1 regularization term\n",
    "remember in the textbook that l1 regularization term can be used for feature selection, since it can generate sparse coefficient matrix (many coefficients will turn to zero.) we can get rid of these features with zero coefficient. Therefore, theoretically, any model that can apply l1 regularization can be used for feature selection.\n",
    "<p>\n",
    "In SVC, logistic we can specify the l1 penalty (in logistic we need to specify the solver as saga). according to sklearn document, we can also set the value of C, which is the inverse of the strength of regularization. the smaller C the fewer features selected'.<\\p>\n",
    "        <p>\n",
    "    When we set penalty to l1 and perform selectfrommodel, the default threshold is 1e-5. otherwise, the default threhold is mean(abs(coef)). I recommend use C to control. </p>\n",
    "In lasso, the model automatically has l1. the default threshold is therefore 1e-5, we can use these 3 models to select feature (since here I am dealing with classification, it is better to use svc or logistics).\n",
    "With Lasso, the higher the alpha parameter, the fewer features selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lr = LogisticRegression( penalty=\"l1\", solver ='saga').fit(x, y)\n",
    "lr = Lasso( alpha=0.3).fit(x, y)\n",
    "#lsvc = LinearSVC( penalty=\"l1\", dual=False).fit(x, y)\n",
    "select = SelectFromModel(lr, prefit=True)\n",
    "x_varname_selected =x_varname[select.get_support(indices=True) ]\n",
    "# use lr.coef_ and select.get_support(indices=True) to check the selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Feature selection using feature importance in decision tree\n",
    "Remember that Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute impurity-based feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer)\n",
    "<p> This implies that in general,tree type classifier can be used for feature selection based on feature importance score they output. Some commonly used classifiers are random forest, decision tree, adaboost with decision stumps. See the following link for more inoformation: https://stats.stackexchange.com/questions/51676/using-adaboost-for-feature-selection </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree = RandomForestRegressor(n_estimators=100, max_leaf_nodes=16, n_jobs=-1).fit(x,y)\n",
    "#tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0).fit(x,y)\n",
    "select = SelectFromModel(tree, prefit=True)\n",
    "x_varname_selected =x_varname[select.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Choosing the best number of base clfs for iterative ensemble method\n",
    "For iterative ensemble method such as adaboost and gradient boost, you can use early stopping to choose the best number of base classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_boost(model):\n",
    "    skfolds = KFold(n_splits=5) \n",
    "    best_n,min_error = [],[]\n",
    "    for train_index, test_index in skfolds.split(x, y):\n",
    "    # for each split, we train a gradient boost and uses early stopping to find the best number of estimator\n",
    "        x_train, y_train = x[train_index],y[train_index]\n",
    "        x_test, y_test =x[test_index], y[test_index]\n",
    "        model.fit(x_train, y_train)\n",
    "        errors = [mean_squared_error(y_test, y_pred) for y_pred in model.staged_predict(x_test)]\n",
    "        best_n.append ( np.argmin(errors) + 1 )\n",
    "        min_error.append (min(errors))\n",
    "    return round(mean(best_n))+1, 1 - mean(min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
